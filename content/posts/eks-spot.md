---
title: "Using spot.io vs Spot instances on EKS Managed NodeGroups"
date: 2020-06-29
draft: true
---

I've been using the offerings from [Spot by NetApp](https://spot.io/) for handling autoscaling of my Kubernetes clusters. I've used both their Elastigroup and their Ocean line of products to do Kubernetes cluster autoscaling, and their controller honestly makes it as simple as possible. You create the Ocean, and your cluster will scale to meet any and all requests that you have. During re:Invent 2020, [AWS announced that EKS managed node groups can now support provisioning instances with the Spot lifecycle](https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-eks-support-ec2-spot-instances-managed-node-groups/). I decided to give the managed node groups a try on a cluster to get a comparison for the user experiences and to see what notable differences are between using one or the other. If you're not familiar with what spot instances are, [AWS does a good job explaining it themselves](https://aws.amazon.com/ec2/spot):

> "Amazon EC2 Spot Instances let you take advantage of unused EC2 capacity in the AWS cloud. Spot Instances are available at up to a 90% discount compared to On-Demand prices."

Simply put, when AWS has extra compute that no one is paying for, you can use it at a discount with the caveat that the instance can be reclaimed with only 2 minutes notice.

**TL;DR: The Spot.io controller is still vastly better... for now**

There was [an update](https://github.com/hashicorp/terraform-provider-aws/pull/16510) to the Terraform provider for the EKS node groups practically instantly from the moment of announcement. This made it really easy to go ahead and test adding a node group to my existing EKS cluster. I built my cluster using [Pulumi EKS](https://github.com/pulumi/pulumi-eks) but Pulumi basically just wraps Terraform and the updates were also released extremely quickly after the announcement.

The managed node groups help you with nodes joining your cluster, and having the nodes gracefully handle termination. With Spot instance types, [EKS will notify the nodes](https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html) of things like rebalancing and spot terminations. This is kind of a big deal. Let's think about how you can get a node to join the cluster normally. All it ultimately takes is that the node needs to prove that it's supposed to join the cluster, which is mainly done in EKS through a combination of the `aws-auth` ConfigMap and security groups. Running in the cluster is a `ConfigMap` which you can think of as arbitrary data, and there's a special `ConfigMap` named `aws-auth` which adds some cluster side rules for how to map the permissions of things like IAM users and IAM roles and what they can access in the cluster. As long as your instance is in the right security groups and has the right instance profile attached to it, you can pretty much join the cluster.

That being said, having some nodes join the cluster is cool and all, but the thing that makes Kubernetes really powerful is that it's a control plane for providing compute resources. In other words, it's a unified API for automatic scaling. This is both scaling horizontally and vertically, and perhaps more importantly, scaling in the sense that it will maintain certain state. If you had 3 worker nodes but one of them died, Kubernetes will take all the action necessary to reconcile the situation and replace that failed worker node. If you combine this with the idea of something like what a public cloud offering does, is that it lets you abstract away compute resources. You no longer have to think about finding a data center to put hardware in and you don't have to worry about things like capacity planning nearly as much anymore. You just ask the cloud for what you need and you can get it instantly. Kubernetes lets you add a layer of abstraction across clouds because it thinks and schedules with raw compute. It thinks in CPUs, disk, and memory. You can define uptime requirements of your applications with things like a [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb/) and help schedule availability across failure zones with [[anti-]affinity policies](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/). So what does it all mean?

Well, it means that in the ideal, perfectly orchestrated cluster, you just have... well... a cloud of compute. Your applications just ask for whatever they need to use and they can get it. Cloud providers will provision everything through the API calls and your applications will be scheduled. Unfortunately, hitting that point of having an ideally orchestrated cluster takes a **lot** of steps. One of those steps is the means by which we can do things like autoscale the workers that are part of a Kubernetes cluster. As new services are being developed, you want your cluster to continue to grow and grow without the developers needing to worry about whether or not their applications will get scheduled. They should just have their applications appear like magic.
